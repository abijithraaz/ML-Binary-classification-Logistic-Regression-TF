{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Binary Classification\n",
        "\n",
        "So far, you've only created regression models. That is, you created models that produced floating-point predictions, such as, \"houses in this neighborhood costs N thousand dollars.\" In this Colab, you'll create and evaluate a binary [classification model](https://developers.google.com/machine-learning/glossary/#classification_model).  That is, you'll create a model that answers a binary question. In this exercise, the binary question will be, **\"Are houses in this neighborhood above a certain price?\"**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuw8rRl9lNuL"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "After doing this Colab, you'll know how to:\n",
        "\n",
        "  * Convert a regression question into a classification question.\n",
        "  * Modify the classification threshold and determine how that modification influences the model.\n",
        "  * Experiment with different classification metrics to determine your model's effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OdC-OglN9D"
      },
      "source": [
        "## The Dataset\n",
        "  \n",
        "Like several of the previous Colabs, this Colab uses the [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iuw6-JOGf7I"
      },
      "source": [
        "## Call the import statements\n",
        "\n",
        "The following code imports the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "# tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "print(\"Ran the import statements.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the datasets from the internet\n",
        "\n",
        "The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the training set\n",
        "\n",
        "train_df.reset_index(inplace=True)\n",
        "test_df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vuAQq0Cvrp"
      },
      "source": [
        "Unlike some of the previous Colabs, the preceding code cell did not scale the label (`median_house_value`).  The following section (\"Normalize values\") provides an alternative approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6y-XcEmk6r"
      },
      "source": [
        "## Normalize values\n",
        "\n",
        "When creating a model with multiple features, the values of each feature should cover roughly the same range.  For example, if one feature's range spans 500 to 100,000 and another feature's range spans 2 to 12, then the model will be difficult or impossible to train. Therefore, you should\n",
        "[normalize](https://developers.google.com/machine-learning/glossary/#normalization) features in a multi-feature model.\n",
        "\n",
        "The following code cell normalizes datasets by converting each raw value (including the label) to its Z-score. A **Z-score** is the number of standard deviations from the mean for a particular raw value. For example, consider a feature having the following characteristics:\n",
        "\n",
        "  * The mean is 60.\n",
        "  * The standard deviation is 10.\n",
        "\n",
        "The raw value 75 would have a Z-score of +1.5:\n",
        "\n",
        "```\n",
        "  Z-score = (75 - 60) / 10 = +1.5\n",
        "```\n",
        "\n",
        "The raw value 38 would have a Z-score of -2.2:\n",
        "\n",
        "```\n",
        "  Z-score = (38 - 60) / 10 = -2.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7nuAHoZIgVI"
      },
      "outputs": [],
      "source": [
        "# Calculate the Z-scores of each column in the training set and\n",
        "# write those Z-scores into a new pandas DataFrame named train_df_norm.\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Examine some of the values of the normalized training set. Notice that most\n",
        "# Z-scores fall between -2 and +2.\n",
        "train_df_norm.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "EwMpaDrT_ErL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python import train\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_df_norm = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\n",
        "test_df_norm = pd.DataFrame(scaler.fit_transform(test_df), columns=train_df.columns)\n",
        "print(train_df_norm.head())\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "PdT18CMw9kuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"scaler_sklearn.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "Ir119nbCHM_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_norm.head()"
      ],
      "metadata": {
        "id": "Kz9jUExibi4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-swmXtWnZGis"
      },
      "source": [
        "## Task 1: Create a binary label\n",
        "\n",
        "In classification problems, the label for every example must be either 0 or 1. Unfortunately, the natural label in the California Housing Dataset, `median_house_value`, contains floating-point values like 80,100 or 85,700 rather than 0s and 1s, while the normalized version of `median_house_values` contains floating-point values primarily between -3 and +3.\n",
        "\n",
        "Your task is to create a new column named `median_house_value_is_high` in both the training set and the test set . If the `median_house_value` is higher than a certain arbitrary value (defined by `threshold`), then set `median_house_value_is_high` to 1. Otherwise, set `median_house_value_is_high` to 0.\n",
        "\n",
        "**Hint:** The cells in the `median_house_value_is_high` column must each hold `1` and `0`, not `True` and `False`. To convert `True` and `False` to  `1` and `0`, call the pandas DataFrame function `astype(float)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4kWfWA8bhKW"
      },
      "outputs": [],
      "source": [
        "threshold = 265000 # This is the 75th percentile for median house values.\n",
        "train_df_norm[\"median_house_value_is_high\"] = (train_df['median_house_value']>threshold).astype(float) #Your code here\n",
        "test_df_norm[\"median_house_value_is_high\"] = (test_df['median_house_value']>threshold).astype(float) #Your code here\n",
        "\n",
        "# Print out a few example cells from the beginning and\n",
        "# middle of the training set, just to make sure that\n",
        "# your code created only 0s and 1s in the newly created\n",
        "# median_house_value_is_high column\n",
        "train_df_norm[\"median_house_value_is_high\"].head(8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_norm.head()\n"
      ],
      "metadata": {
        "id": "pXRXmIXqeWmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kir8UTUXSV8"
      },
      "source": [
        "## Represent features as input layers\n",
        "\n",
        "This code cell specifies the features, `median_income` and ` total_rooms`, that you'll ultimately train the model on. These [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects are instantiated as Keras tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tmmZIDw4JEC"
      },
      "outputs": [],
      "source": [
        "inputs = {\n",
        "# Features used to train the model on.\n",
        "  'median_income': tf.keras.Input(shape=(1,)),\n",
        "  'total_rooms': tf.keras.Input(shape=(1,))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code cell defines two functions:\n",
        "\n",
        "  * `create_model(inputs, learning_rate, METRICS)`, which defines the model's\n",
        "    topography.\n",
        "  * `train_model(model, dataset, epochs, label_name, batch_size, shuffle)`, uses input features and labels to train the model.\n",
        "\n",
        "Prior exercises used [ReLU](https://developers.google.com/machine-learning/glossary#ReLU) as the [activation function](https://developers.google.com/machine-learning/glossary#activation-function). By contrast, this exercise uses [sigmoid](https://developers.google.com/machine-learning/glossary#sigmoid-function) as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pedD5GhlDC-y"
      },
      "outputs": [],
      "source": [
        "#title Define the functions that create and train a model.\n",
        "def create_model(my_inputs, my_learning_rate, METRICS):\n",
        "  # Use a Concatenate layer to concatenate the input layers into a single tensor.\n",
        "  # as input for the Dense layer. Ex: [input_1[0][0], input_2[0][0]]\n",
        "  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())\n",
        "  dense = layers.Dense(units=1, name='dense_layer', activation=tf.sigmoid)\n",
        "  dense_output = dense(concatenated_inputs)\n",
        "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
        "  my_outputs = {\n",
        "    'dense': dense_output,\n",
        "  }\n",
        "  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
        "\n",
        "  # Call the compile method to construct the layers into a model that\n",
        "  # TensorFlow can execute.  Notice that we're using a different loss\n",
        "  # function for classification than for regression.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=METRICS)\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None, shuffle=True):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature.  Here, we're passing\n",
        "  # every column in the dataset. Note that the feature_layer will filter\n",
        "  # away most of those columns, leaving only the desired columns and their\n",
        "  # representations as features.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=shuffle)\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the classification metric for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist\n",
        "\n",
        "print(\"Defined the create_model and train_model functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Define a plotting function\n",
        "\n",
        "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) function plots one or more curves, showing how various classification metrics change with each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "outputs": [],
      "source": [
        "#title Define the plotting function.\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"\n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Defined the plot_curve function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8crSCCVf6gm"
      },
      "source": [
        "## Task 3: Add precision and recall as metrics\n",
        "\n",
        "Relying solely on accuracy, particularly for a class-imbalanced data set (like ours), can be a poor way to judge a classification model.  Modify the code in the following code cell to enable the model to measure not only accuracy but also precision and recall. We have\n",
        "added accuracy and precision; your task is to add recall. See the [TensorFlow Reference](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) for details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-k1MD2XArmO"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "classification_threshold = 0.35\n",
        "label_name = \"median_house_value_is_high\"\n",
        "\n",
        "# Modify the following definition of METRICS to generate\n",
        "# not only accuracy and precision, but also recall:\n",
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy',\n",
        "                                      threshold=classification_threshold),\n",
        "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision'\n",
        "                                 ),\n",
        "      tf.keras.metrics.Recall(thresholds=classification_threshold, name='recall')  # write code here\n",
        "]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(inputs, learning_rate, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train_df_norm, epochs,\n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot metrics vs. epochs\n",
        "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model downloading\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "q0S2H0bukOSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.save('house_value_model.keras')\n"
      ],
      "metadata": {
        "id": "g_wc4firkChc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.summary()"
      ],
      "metadata": {
        "id": "-ZHd0N_N3iv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNqWWos_zyk"
      },
      "source": [
        "## Evaluate the model against the test set\n",
        "\n",
        "At the end of model training, you ended up with a certain accuracy against the *training set*. Invoke the following code cell to determine your model's accuracy against the *test set*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "load_model = tf.keras.models.load_model('house_value_model.keras')\n",
        "\n",
        "features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "label = np.array(features.pop(label_name))\n",
        "\n",
        "load_model.evaluate(x = features, y = label, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "WmkdCp54ndUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary Classification.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
